{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92be8215-820b-490a-a5dd-fe96eb846dba",
   "metadata": {},
   "source": [
    "# ECE9039 MACHINE LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a45222-985d-49d3-8fcb-9625be0ab111",
   "metadata": {
    "creator": "Haoran Wang"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5486ee10-b40e-451c-a785-47e7124e59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image, ImageOps\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8bd8f5-1215-49a6-bdf7-f4185ebaec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Name: /physical_device:GPU:0 Type: GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"Type:\", gpu.device_type)\n",
    "else:\n",
    "    print(\"No GPU is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afabeb4-1b10-4432-b0cd-bc878f9160b1",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1e49ab-35ed-483c-b7ac-3cca694ce19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(dataset_path, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Loop through each subfolder in the dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if root == dataset_path:\n",
    "            continue\n",
    "        label = os.path.basename(root)\n",
    "        count = 0\n",
    "        for file in sorted(files):\n",
    "            if file.lower().endswith(\".jpg\"):\n",
    "                # Construct the full path to the image file\n",
    "                file_path = os.path.join(root, file)\n",
    "                image = Image.open(file_path).convert('RGB').resize(target_size)\n",
    "                images.append(np.array(image))\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "                \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6802bdb1-9b5b-47d8-b3c9-7bb98f2f03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (4752, 224, 224, 3)\n",
      "Labels count: 9\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'RealWaste/'\n",
    "images, labels = load_images_and_labels(dataset_path)\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels count:\", len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c02f3f3-547d-4727-a13f-887874a2183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b992015-99fa-4546-b5a9-205b2401f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plastic: 921\n",
      "Vegetation: 436\n",
      "Paper: 500\n",
      "Textile Trash: 318\n",
      "Metal: 790\n",
      "Miscellaneous Trash: 495\n",
      "Cardboard: 461\n",
      "Food Organics: 411\n",
      "Glass: 420\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d83db-26f8-4f5e-985b-775b32554e24",
   "metadata": {},
   "source": [
    "### Isolate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0b6019-74c8-40e3-b8b5-e5e51332c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_test_samples_per_class = 100\n",
    "target_num_of_train_per_class = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56de2528-973a-4b3e-b5eb-4de662a66b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(labels)\n",
    "test_indices = []\n",
    "train_indices = []\n",
    "\n",
    "for class_label in classes:\n",
    "    # Find the indices of images belonging to the current class\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "    \n",
    "    # Shuffle the indices to ensure random selection\n",
    "    np.random.shuffle(class_indices)\n",
    "    test_indices.extend(class_indices[:num_of_test_samples_per_class])\n",
    "    train_indices.extend(class_indices[num_of_test_samples_per_class:])\n",
    "\n",
    "test_images = images[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "train_images = images[train_indices]\n",
    "train_labels = labels[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f40f75-9699-4dee-bb36-001fe4e734a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3852, 224, 224, 3)\n",
      "(3852,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905f5175-98c2-44fd-a172-971083dabc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185534be-05df-4946-91ec-ef15c0c86170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardboard: 361\n",
      "Food Organics: 311\n",
      "Glass: 320\n",
      "Metal: 690\n",
      "Miscellaneous Trash: 395\n",
      "Paper: 400\n",
      "Plastic: 821\n",
      "Textile Trash: 218\n",
      "Vegetation: 336\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(train_labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfe150d-1c84-4c16-95ec-72e6da9a5cf6",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eea8bba-6e6b-42ce-9bf7-7c4ed46f20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8e9fd2-a627-4a97-931c-f95ec40e16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for i in label_counts:\n",
    "    desired = target_num_of_train_per_class-label_counts[i]\n",
    "    dict[i] = desired if desired > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92fea3bc-f311-4969-b7c3-d88b63732716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cardboard': 439, 'Food Organics': 489, 'Glass': 480, 'Metal': 110, 'Miscellaneous Trash': 405, 'Paper': 400, 'Plastic': 0, 'Textile Trash': 582, 'Vegetation': 464}\n",
      "3369\n"
     ]
    }
   ],
   "source": [
    "print(dict)\n",
    "print(sum(dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a87ae34-3565-4635-b299-174826a7ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b669e170-54a7-46a7-85bf-a401ad1280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "def generate_augmented_image(image, label): #,index\n",
    "    for x_batch in datagen.flow(image.reshape((1,)+image.shape),batch_size=1): # ,save_to_dir='Preview', save_prefix=str(index), save_format='jpg'\n",
    "        augmented_images.append(x_batch[0])\n",
    "        augmented_labels.append(label)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c53f1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while sum(dict.values()) > 0:\n",
    "    j = i % len(train_images)\n",
    "    if dict[train_labels[j]] > 0:\n",
    "        dict[train_labels[j]]-=1 \n",
    "        generate_augmented_image(train_images[j],train_labels[j])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6322174f-a222-4558-aaa0-a2375621dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3369, 224, 224, 3)\n",
      "(3369,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(augmented_images).shape)\n",
    "print(np.array(augmented_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dfe2b66-64f1-45ef-9b60-63ccd497a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_images = np.concatenate((train_images,augmented_images))\n",
    "train_aug_labels = np.concatenate((train_labels,augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de915246-b87e-47da-b1fc-a36eeceeeea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7221, 224, 224, 3)\n",
      "(7221,)\n",
      "(900, 224, 224, 3)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "print(train_aug_images.shape)\n",
    "print(train_aug_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e941de-77cf-4140-a012-950a2026a750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardboard: 800\n",
      "Food Organics: 800\n",
      "Glass: 800\n",
      "Metal: 800\n",
      "Miscellaneous Trash: 800\n",
      "Paper: 800\n",
      "Plastic: 821\n",
      "Textile Trash: 800\n",
      "Vegetation: 800\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(train_aug_labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2274b48d-6cb0-4826-8b37-aad558d71c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_aug_labels_encoded = label_encoder.fit_transform(train_aug_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f547cc-550e-4127-9043-8cd176f79422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7221, 224, 224, 3)\n",
      "(7221,)\n",
      "(450, 224, 224, 3)\n",
      "(450,)\n",
      "(450, 224, 224, 3)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_aug_images\n",
    "y_train = train_aug_labels_encoded\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(test_images, test_labels_encoded, test_size=0.5, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f7f068a-1fea-4393-a8c3-8eb8f714752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_images = None\n",
    "train_aug_labels = None\n",
    "train_aug_labels_encoded = None\n",
    "#test_images = None\n",
    "#test_labels = None\n",
    "#test_labels_encoded = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df159c0c-b070-4239-ae38-4bb2b45057b9",
   "metadata": {},
   "source": [
    "## Feature Extraction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33ceaa-3209-43f3-bd85-c55f338877a8",
   "metadata": {},
   "source": [
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bebe17c-fcce-4f92-8662-b4f446e37f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96c3837d-2618-4665-a84f-3ba650053978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3852, 224, 224, 3)\n",
      "(3852,)\n",
      "(900, 224, 224, 3)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da560ba-5876-4627-b5a2-eb92177f2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(images, labels, num_strongest=50):\n",
    "    sift = cv2.SIFT_create(num_strongest)\n",
    "    features = []\n",
    "    new_labels = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(len(images)):\n",
    "        gray = cv2.cvtColor(images[i], cv2.COLOR_RGB2GRAY)\n",
    "        pts, des = sift.detectAndCompute(gray, None)\n",
    "        j = 0\n",
    "        for j in range(num_strongest):\n",
    "            new_labels.append(labels[i])\n",
    "            if j < len(des):\n",
    "                features.append(np.array(des[j]))\n",
    "            else:\n",
    "                features.append(np.zeros((128,)))\n",
    "        \n",
    "    return np.array(features),np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ca148c-98fc-486a-8ba2-0564dc046a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features2(images):\n",
    "    # Initialize SIFT detector and select strongest 50\n",
    "    sift = cv2.SIFT_create(50)\n",
    "    features = []\n",
    "    \n",
    "    for img in images:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        # Detect SIFT features and compute descriptors\n",
    "        pts, des = sift.detectAndCompute(gray, None)\n",
    "        if des is not None:\n",
    "            # Simple feature aggregation by taking the mean\n",
    "            des_mean = np.mean(des, axis=0)\n",
    "        else:\n",
    "            # If no features are found, use a zero vector\n",
    "            des_mean = np.zeros((128,))\n",
    "        features.append(des_mean)\n",
    "        \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53fbca72-06c0-4dbd-9efa-139cf8f875d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_encoded = label_encoder.fit_transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c63f72bf-3e7f-4e25-bfd0-2e6c40d66c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_des, y_train_des = extract_sift_features(train_images, train_labels_encoded)\n",
    "#test_features, test_featured_labels = extract_sift_features(test_images, test_labels_encoded)\n",
    "#X_val_des, X_test_des, y_val_des, y_test_des = train_test_split(test_features, test_featured_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e82f176e-b060-407c-92c4-0f1ee64c4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_des = extract_sift_features2(train_images)\n",
    "y_train_des = train_labels_encoded\n",
    "test_features = extract_sift_features2(test_images)\n",
    "X_val_des, X_test_des, y_val_des, y_test_des = train_test_split(test_features, test_labels_encoded, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3978106-21c6-4d4f-819a-162da2e192fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3852, 128)\n",
      "(3852,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_des.shape)\n",
    "print(y_train_des.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c9d5383-8fae-4eff-9416-02c6df653101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 128)\n",
      "(450,)\n",
      "(450, 128)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_val_des.shape)\n",
    "print(y_val_des.shape)\n",
    "print(X_test_des.shape)\n",
    "print(y_test_des.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca3a1f-1e41-4f7a-84f7-bcc838459407",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db758e45-8178-42c1-9432-d359f922ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69527e-9b36-4dde-b154-e55cb878e5a6",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587c686-42dd-41e8-9c83-46203146f02a",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9b279-29e3-475a-a742-1932e9f35e1b",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_des = scaler.fit_transform(X_train_des)\n",
    "X_test_des = scaler.fit_transform(X_test_des)\n",
    "\n",
    "pca = PCA(n_components=64)\n",
    "X_train_des = pca.fit_transform(X_train_des)\n",
    "X_test_des = pca.fit_transform(X_test_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6cde9-4d82-4644-babd-0aaacbecc422",
   "metadata": {},
   "source": [
    "## Models use feature descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64e1e4-cd9d-4ce0-b013-cc10db1430ef",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d0f205a0-5703-43ac-a973-281d8f7d52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from ray import train\n",
    "from ray.tune.schedulers import HyperBandScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74b039a8-7d6d-408b-9820-dd721f8635d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(config):    \n",
    "    model = RandomForestClassifier(n_estimators=config[\"n_estimators\"], max_depth=config[\"max_depth\"])\n",
    "    model.fit(X_train_des, y_train_des)\n",
    "    preds = model.predict(X_test_des)\n",
    "    accuracy = accuracy_score(y_test_des, preds)\n",
    "    \n",
    "    train.report({'accuracy': accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0be89e1e-57a3-42b2-90f6-9a5542866b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_estimators\": tune.choice([10, 50, 100, 300, 500, 700]),\n",
    "    \"max_depth\": tune.choice([5, 10, 20, 30, 40, 50]),\n",
    "}\n",
    "\n",
    "scheduler = HyperBandScheduler(metric=\"accuracy\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d20228f-f4d4-4d20-9db0-7dfffe1c5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:29:11,880\tINFO tune.py:646 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-05 11:30:45</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:33.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.1/31.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using HyperBand: num_stopped=0 total_brackets=2<br>Round #0:<br>  Bracket(Max Size (n)=5, Milestone (r)=81, completed=100.0%): {TERMINATED: 5} <br>  Bracket(Max Size (n)=3, Milestone (r)=81, completed=100.0%): {TERMINATED: 5} <br>Logical resource usage: 1.0/16 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  n_estimators</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_random_forest_7fb8c_00000</td><td>TERMINATED</td><td>127.0.0.1:28820</td><td style=\"text-align: right;\">         50</td><td style=\"text-align: right;\">            10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.241397</td><td style=\"text-align: right;\">  0.262222</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00001</td><td>TERMINATED</td><td>127.0.0.1:20772</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.29238 </td><td style=\"text-align: right;\">  0.237778</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00002</td><td>TERMINATED</td><td>127.0.0.1:21288</td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           300</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        7.05657 </td><td style=\"text-align: right;\">  0.32    </td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00003</td><td>TERMINATED</td><td>127.0.0.1:1664 </td><td style=\"text-align: right;\">         40</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       11.8237  </td><td style=\"text-align: right;\">  0.333333</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00004</td><td>TERMINATED</td><td>127.0.0.1:16776</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">            10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.18516 </td><td style=\"text-align: right;\">  0.253333</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00005</td><td>TERMINATED</td><td>127.0.0.1:29108</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">           100</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.39034 </td><td style=\"text-align: right;\">  0.311111</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00006</td><td>TERMINATED</td><td>127.0.0.1:29268</td><td style=\"text-align: right;\">         30</td><td style=\"text-align: right;\">           700</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       16.4376  </td><td style=\"text-align: right;\">  0.322222</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00007</td><td>TERMINATED</td><td>127.0.0.1:11048</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">           500</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        9.22199 </td><td style=\"text-align: right;\">  0.291111</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00008</td><td>TERMINATED</td><td>127.0.0.1:29460</td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">           300</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.20349 </td><td style=\"text-align: right;\">  0.244444</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00009</td><td>TERMINATED</td><td>127.0.0.1:26868</td><td style=\"text-align: right;\">         20</td><td style=\"text-align: right;\">            10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.233382</td><td style=\"text-align: right;\">  0.257778</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th style=\"text-align: right;\">  accuracy</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_random_forest_7fb8c_00000</td><td style=\"text-align: right;\">  0.262222</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00001</td><td style=\"text-align: right;\">  0.237778</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00002</td><td style=\"text-align: right;\">  0.32    </td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00003</td><td style=\"text-align: right;\">  0.333333</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00004</td><td style=\"text-align: right;\">  0.253333</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00005</td><td style=\"text-align: right;\">  0.311111</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00006</td><td style=\"text-align: right;\">  0.322222</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00007</td><td style=\"text-align: right;\">  0.291111</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00008</td><td style=\"text-align: right;\">  0.244444</td></tr>\n",
       "<tr><td>train_random_forest_7fb8c_00009</td><td style=\"text-align: right;\">  0.257778</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:30:45,433\tINFO tune.py:1144 -- Total run time: 93.55 seconds (93.53 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    train_random_forest,\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6170e914-b667-4d9b-b513-9f56a9ff3f25",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5462456-6dad-4fc8-89fc-b20aff6ac38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9751fc59-8546-4c57-9444-506d2416ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Accuracy: 0.38222222222222224\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_des)\n",
    "X_test_scaled = scaler.transform(X_test_des)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train_scaled, y_train_des)\n",
    "y_pred = svm_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test_des, y_pred)\n",
    "print(f\"SVM Classification Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fc32-5f6a-45c7-ae3c-e73babb9105e",
   "metadata": {},
   "source": [
    "## Models use raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64152e27-4d6f-40a1-94d7-3a284a103d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot(history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss function Plot')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2191e-965e-4d05-9147-22b8cacf923d",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0839e94b-348a-4c83-9bd9-2f2cf5714305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch, lr):\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    return lr * (drop ** (epoch // epochs_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bab93d9d-b761-4975-b9e0-7ab238caadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_pretrained_model(X_train_t,y_train_t,X_val_t,y_val_t,base_model,filename,initial_lr):\n",
    "    base_model.trainable = True\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        #Dense(512, activation='relu'),\n",
    "        Dense(9, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(step_decay)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, mode='min')\n",
    "    model_checkpoint = ModelCheckpoint(filepath=filename, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=initial_lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_t, y_train_t, epochs=50,validation_data=(X_val_t, y_val_t), batch_size=32, callbacks=[lr_scheduler, early_stopping, model_checkpoint])\n",
    "    return model,history    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8b606-a295-4f3a-b3fc-a39d1eeda3b9",
   "metadata": {},
   "source": [
    "#### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b432b1cf-5c66-4e64-8f40-63a021c24330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6df938-129f-4363-9e5a-e8c392bdf810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "226/226 [==============================] - 49s 177ms/step - loss: 2.0660 - accuracy: 0.2227 - val_loss: 1.6954 - val_accuracy: 0.3778\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.37778, saving model to VGG19_t.h5\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 37s 164ms/step - loss: 1.5748 - accuracy: 0.4210 - val_loss: 1.2931 - val_accuracy: 0.5267\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.37778 to 0.52667, saving model to VGG19_t.h5\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 38s 169ms/step - loss: 1.2350 - accuracy: 0.5358 - val_loss: 1.2288 - val_accuracy: 0.5489\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52667 to 0.54889, saving model to VGG19_t.h5\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 38s 167ms/step - loss: 1.0592 - accuracy: 0.5959 - val_loss: 1.0370 - val_accuracy: 0.6333\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.54889 to 0.63333, saving model to VGG19_t.h5\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 38s 170ms/step - loss: 0.8453 - accuracy: 0.6892 - val_loss: 0.8301 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.63333 to 0.70000, saving model to VGG19_t.h5\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 37s 162ms/step - loss: 0.6898 - accuracy: 0.7496 - val_loss: 0.8927 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.70000\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.5861 - accuracy: 0.7912 - val_loss: 0.7011 - val_accuracy: 0.7667\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.70000 to 0.76667, saving model to VGG19_t.h5\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 36s 161ms/step - loss: 0.5109 - accuracy: 0.8190 - val_loss: 0.8982 - val_accuracy: 0.7022\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.76667\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.4393 - accuracy: 0.8460 - val_loss: 0.6499 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.76667 to 0.78667, saving model to VGG19_t.h5\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 36s 160ms/step - loss: 0.3742 - accuracy: 0.8693 - val_loss: 0.6452 - val_accuracy: 0.7867\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.78667\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.1910 - accuracy: 0.9324 - val_loss: 0.6249 - val_accuracy: 0.8200\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.78667 to 0.82000, saving model to VGG19_t.h5\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 36s 158ms/step - loss: 0.1439 - accuracy: 0.9497 - val_loss: 0.5606 - val_accuracy: 0.8311\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.82000 to 0.83111, saving model to VGG19_t.h5\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.1399 - accuracy: 0.9489 - val_loss: 0.7036 - val_accuracy: 0.8111\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.83111\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 36s 160ms/step - loss: 0.1076 - accuracy: 0.9637 - val_loss: 0.5669 - val_accuracy: 0.8622\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.83111 to 0.86222, saving model to VGG19_t.h5\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 36s 161ms/step - loss: 0.0907 - accuracy: 0.9686 - val_loss: 0.6982 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.86222\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.0729 - accuracy: 0.9745 - val_loss: 0.7275 - val_accuracy: 0.8422\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.86222\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 36s 158ms/step - loss: 0.0731 - accuracy: 0.9740 - val_loss: 0.7416 - val_accuracy: 0.8178\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.86222\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.0806 - accuracy: 0.9729 - val_loss: 0.9600 - val_accuracy: 0.7956\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.86222\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.0798 - accuracy: 0.9733 - val_loss: 0.6796 - val_accuracy: 0.8756\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.86222 to 0.87556, saving model to VGG19_t.h5\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 0.0658 - accuracy: 0.9777 - val_loss: 0.7555 - val_accuracy: 0.8356\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.87556\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 36s 161ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.9055 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.87556\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 36s 160ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.7773 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00022: val_accuracy improved from 0.87556 to 0.88444, saving model to VGG19_t.h5\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 8.5778e-04 - accuracy: 1.0000 - val_loss: 0.9066 - val_accuracy: 0.8711\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.88444\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 37s 162ms/step - loss: 3.9706e-04 - accuracy: 1.0000 - val_loss: 0.9256 - val_accuracy: 0.8778\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.88444\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 37s 165ms/step - loss: 2.4112e-04 - accuracy: 1.0000 - val_loss: 0.9714 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.88444\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 37s 164ms/step - loss: 1.7296e-04 - accuracy: 1.0000 - val_loss: 1.0095 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.88444\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 36s 159ms/step - loss: 1.3260e-04 - accuracy: 1.0000 - val_loss: 1.0402 - val_accuracy: 0.8711\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.88444\n",
      "Epoch 00027: early stopping\n",
      "15/15 [==============================] - 1s 43ms/step - loss: 1.2037 - accuracy: 0.8644\n",
      "Test loss: 1.2036923170089722, Test accuracy: 0.8644444346427917\n"
     ]
    }
   ],
   "source": [
    "[VGG19_model, VGG19_history] = train_with_pretrained_model(X_train, y_train, X_val, y_val, VGG19(include_top=False, weights='imagenet', input_shape=(224,224,3)), \"VGG19_t.h5\", 0.0001)\n",
    "loss, accuracy = VGG19_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399bfb3-1cef-4c0a-9490-c1f6b75aab8f",
   "metadata": {},
   "source": [
    "#### DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1654712-b6fb-40a5-aa90-ce5f3fa116bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7635d65-c8c5-4b57-9aba-112a880fcd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "226/226 [==============================] - 48s 153ms/step - loss: 0.8595 - accuracy: 0.7092 - val_loss: 1.7597 - val_accuracy: 0.5244\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.52444, saving model to DenseNet121_t.h5\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 30s 134ms/step - loss: 0.4650 - accuracy: 0.8384 - val_loss: 2.1223 - val_accuracy: 0.3756\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.52444\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 29s 128ms/step - loss: 0.3474 - accuracy: 0.8755 - val_loss: 1.7762 - val_accuracy: 0.5533\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52444 to 0.55333, saving model to DenseNet121_t.h5\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.2445 - accuracy: 0.9139 - val_loss: 1.3933 - val_accuracy: 0.6489\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.55333 to 0.64889, saving model to DenseNet121_t.h5\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.2657 - accuracy: 0.9056 - val_loss: 2.2046 - val_accuracy: 0.4822\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.64889\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.1770 - accuracy: 0.9374 - val_loss: 0.9294 - val_accuracy: 0.7711\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.64889 to 0.77111, saving model to DenseNet121_t.h5\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.1479 - accuracy: 0.9488 - val_loss: 1.5624 - val_accuracy: 0.6844\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.77111\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.1211 - accuracy: 0.9591 - val_loss: 15.1256 - val_accuracy: 0.1600\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.77111\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.1437 - accuracy: 0.9517 - val_loss: 2.2086 - val_accuracy: 0.5600\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.77111\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.1156 - accuracy: 0.9590 - val_loss: 2.9682 - val_accuracy: 0.5022\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.77111\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.3146 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.77111 to 0.91556, saving model to DenseNet121_t.h5\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0065 - accuracy: 0.9992 - val_loss: 0.2771 - val_accuracy: 0.9244\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.91556 to 0.92444, saving model to DenseNet121_t.h5\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.2682 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.92444\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.2712 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.92444\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.2650 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.92444\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2672 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.92444\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2646 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.92444\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.2678 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.92444\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.2671 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.92444\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.2696 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.92444\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.2652 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.92444\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.2623 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.92444\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.2642 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.92444\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0025 - accuracy: 0.9999 - val_loss: 0.2690 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.92444\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.2671 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.92444\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2673 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.92444\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0027 - accuracy: 0.9999 - val_loss: 0.2663 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.92444\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.92444\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.2656 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.92444\n",
      "Epoch 30/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.2642 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.92444\n",
      "Epoch 31/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.92444\n",
      "Epoch 32/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.92444\n",
      "Epoch 33/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.92444\n",
      "Epoch 34/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0026 - accuracy: 0.9999 - val_loss: 0.2661 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.92444\n",
      "Epoch 35/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.2700 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.92444\n",
      "Epoch 36/50\n",
      "226/226 [==============================] - 31s 137ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2662 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.92444\n",
      "Epoch 37/50\n",
      "226/226 [==============================] - 31s 136ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.2650 - val_accuracy: 0.9222\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.92444\n",
      "Epoch 00037: early stopping\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.2852 - accuracy: 0.9200\n",
      "Test loss: 0.28520870208740234, Test accuracy: 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "[DenseNet121_model, DenseNet121_history] = train_with_pretrained_model(X_train, y_train, X_val, y_val, DenseNet121(include_top=False, weights='imagenet', input_shape=(224,224,3)), \"DenseNet121_t.h5\", 0.001)\n",
    "loss, accuracy = DenseNet121_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a071b1-466f-43bf-95c6-82cd1717f711",
   "metadata": {},
   "source": [
    "#### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86e99510-2e11-44af-bb32-a6c78082a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31878bc0-8fc2-476b-a282-d27167c27d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 16s 0us/step\n",
      "Epoch 1/50\n",
      "226/226 [==============================] - 31s 98ms/step - loss: 0.9789 - accuracy: 0.6639 - val_loss: 2.8952 - val_accuracy: 0.3267\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.32667, saving model to EfficientNetB7_t.h5\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.5936 - accuracy: 0.7948 - val_loss: 1.5709 - val_accuracy: 0.5267\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.32667 to 0.52667, saving model to EfficientNetB7_t.h5\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.4476 - accuracy: 0.8445 - val_loss: 1.6033 - val_accuracy: 0.6000\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.52667 to 0.60000, saving model to EfficientNetB7_t.h5\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.3354 - accuracy: 0.8813 - val_loss: 1.1666 - val_accuracy: 0.7000\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.60000 to 0.70000, saving model to EfficientNetB7_t.h5\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.2504 - accuracy: 0.9134 - val_loss: 1.2573 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.70000\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.2354 - accuracy: 0.9176 - val_loss: 0.8304 - val_accuracy: 0.7578\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.70000 to 0.75778, saving model to EfficientNetB7_t.h5\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 20s 86ms/step - loss: 0.1688 - accuracy: 0.9417 - val_loss: 1.0682 - val_accuracy: 0.7467\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.75778\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.1626 - accuracy: 0.9456 - val_loss: 5.3175 - val_accuracy: 0.3156\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.75778\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 19s 86ms/step - loss: 0.1610 - accuracy: 0.9431 - val_loss: 2.1046 - val_accuracy: 0.5467\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.75778\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.1096 - accuracy: 0.9629 - val_loss: 1.5667 - val_accuracy: 0.6556\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.75778\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.5616 - val_accuracy: 0.8578\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.75778 to 0.85778, saving model to EfficientNetB7_t.h5\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 19s 81ms/step - loss: 0.0110 - accuracy: 0.9967 - val_loss: 0.3733 - val_accuracy: 0.9044\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.85778 to 0.90444, saving model to EfficientNetB7_t.h5\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0042 - accuracy: 0.9996 - val_loss: 0.3766 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.90444\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.3731 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.90444\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.3783 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.90444\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 0.3850 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.90444\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3881 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.90444\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 20s 86ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.3860 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.90444\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.90444\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.3845 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.90444\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.3854 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.90444\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.3857 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.90444\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.3850 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.90444\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.3850 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.90444\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.3888 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.90444\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.3848 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.90444\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 18s 82ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.3859 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.90444\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.3899 - val_accuracy: 0.8978\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.90444\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.3872 - val_accuracy: 0.9000\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.90444\n",
      "Epoch 00029: early stopping\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.2969 - accuracy: 0.9178\n",
      "Test loss: 0.2968514561653137, Test accuracy: 0.9177777767181396\n"
     ]
    }
   ],
   "source": [
    "[InceptionV3_model, InceptionV3_history] = train_with_pretrained_model(X_train, y_train, X_val, y_val, InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3)), \"InceptionV3_t.h5\", 0.001)\n",
    "loss, accuracy = InceptionV3_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74bdb1-3c3e-4b10-a4e2-c67adac6a624",
   "metadata": {},
   "source": [
    "#### MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8550bd6d-529b-41f4-abe2-48dabf79bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8782bc8-02b5-48e6-885f-f2f2cad6543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 13s 1us/step\n",
      "Epoch 1/50\n",
      "226/226 [==============================] - 24s 84ms/step - loss: 0.7436 - accuracy: 0.7567 - val_loss: 3.4404 - val_accuracy: 0.4378\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43778, saving model to MobileNet_t.h5\n",
      "Epoch 2/50\n",
      "226/226 [==============================] - 18s 82ms/step - loss: 0.2858 - accuracy: 0.9015 - val_loss: 1.1461 - val_accuracy: 0.7044\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.43778 to 0.70444, saving model to MobileNet_t.h5\n",
      "Epoch 3/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.2044 - accuracy: 0.9313 - val_loss: 1.0431 - val_accuracy: 0.7333\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.70444 to 0.73333, saving model to MobileNet_t.h5\n",
      "Epoch 4/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.1897 - accuracy: 0.9331 - val_loss: 1.1143 - val_accuracy: 0.7533\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.73333 to 0.75333, saving model to MobileNet_t.h5\n",
      "Epoch 5/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.1417 - accuracy: 0.9517 - val_loss: 1.5110 - val_accuracy: 0.6889\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.75333\n",
      "Epoch 6/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.1202 - accuracy: 0.9591 - val_loss: 0.8504 - val_accuracy: 0.7844\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.75333 to 0.78444, saving model to MobileNet_t.h5\n",
      "Epoch 7/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0966 - accuracy: 0.9698 - val_loss: 1.0147 - val_accuracy: 0.7889\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.78444 to 0.78889, saving model to MobileNet_t.h5\n",
      "Epoch 8/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0917 - accuracy: 0.9673 - val_loss: 0.8127 - val_accuracy: 0.8000\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.78889 to 0.80000, saving model to MobileNet_t.h5\n",
      "Epoch 9/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.1213 - accuracy: 0.9601 - val_loss: 1.0926 - val_accuracy: 0.7200\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.80000\n",
      "Epoch 10/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0760 - accuracy: 0.9749 - val_loss: 0.9665 - val_accuracy: 0.7978\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.80000\n",
      "Epoch 11/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0297 - accuracy: 0.9896 - val_loss: 0.4029 - val_accuracy: 0.8956\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.80000 to 0.89556, saving model to MobileNet_t.h5\n",
      "Epoch 12/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.2957 - val_accuracy: 0.9133\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.89556 to 0.91333, saving model to MobileNet_t.h5\n",
      "Epoch 13/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.2886 - val_accuracy: 0.9111\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.91333\n",
      "Epoch 14/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2911 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.91333 to 0.91556, saving model to MobileNet_t.h5\n",
      "Epoch 15/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.2899 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.91556\n",
      "Epoch 16/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.2904 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.91556\n",
      "Epoch 17/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.2903 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.91556\n",
      "Epoch 18/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.91556\n",
      "Epoch 19/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2888 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.91556\n",
      "Epoch 20/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2884 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.91556\n",
      "Epoch 21/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2875 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.91556\n",
      "Epoch 22/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2885 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.91556\n",
      "Epoch 23/50\n",
      "226/226 [==============================] - 18s 81ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.2876 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.91556\n",
      "Epoch 24/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2885 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.91556\n",
      "Epoch 25/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2895 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.91556\n",
      "Epoch 26/50\n",
      "226/226 [==============================] - 20s 86ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2889 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.91556\n",
      "Epoch 27/50\n",
      "226/226 [==============================] - 20s 88ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2900 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.91556\n",
      "Epoch 28/50\n",
      "226/226 [==============================] - 20s 88ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.2872 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.91556\n",
      "Epoch 29/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2882 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.91556\n",
      "Epoch 30/50\n",
      "226/226 [==============================] - 20s 88ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.2890 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.91556\n",
      "Epoch 31/50\n",
      "226/226 [==============================] - 20s 88ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.91556\n",
      "Epoch 32/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.2899 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.91556\n",
      "Epoch 33/50\n",
      "226/226 [==============================] - 19s 82ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.2888 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.91556\n",
      "Epoch 34/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2887 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.91556\n",
      "Epoch 35/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.91556\n",
      "Epoch 36/50\n",
      "226/226 [==============================] - 19s 86ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.2892 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.91556\n",
      "Epoch 37/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0014 - accuracy: 0.9999 - val_loss: 0.2893 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.91556\n",
      "Epoch 38/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2887 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.91556\n",
      "Epoch 39/50\n",
      "226/226 [==============================] - 19s 83ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.91556\n",
      "Epoch 40/50\n",
      "226/226 [==============================] - 20s 87ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.2883 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.91556\n",
      "Epoch 41/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.91556\n",
      "Epoch 42/50\n",
      "226/226 [==============================] - 19s 84ms/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.2889 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.91556\n",
      "Epoch 43/50\n",
      "226/226 [==============================] - 19s 85ms/step - loss: 0.0016 - accuracy: 0.9999 - val_loss: 0.2891 - val_accuracy: 0.9156\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.91556\n",
      "Epoch 00043: early stopping\n",
      "15/15 [==============================] - 0s 11ms/step - loss: 0.4457 - accuracy: 0.9089\n",
      "Test loss: 0.44568029046058655, Test accuracy: 0.9088888764381409\n"
     ]
    }
   ],
   "source": [
    "[MobileNet_model, MobileNet_history] = train_with_pretrained_model(X_train, y_train, X_val, y_val, MobileNet(include_top=False, weights='imagenet', input_shape=(224,224,3)), \"MobileNet_t.h5\", 0.001)\n",
    "loss, accuracy = MobileNet_model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9c7bd-9ee1-4c3a-904f-4fb3d639806b",
   "metadata": {},
   "source": [
    "### Ensemble (soft voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d2b4de2e-507e-4bcb-85b8-cef92b7a280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(list_of_models, X_test_t, y_test_t):\n",
    "    true = 0\n",
    "    false = 0\n",
    "    for i in range(len(X_test_t)):\n",
    "        probs = []\n",
    "        for model in list_of_models:\n",
    "            img = np.expand_dims(X_test_t[i], axis=0)\n",
    "            res = model.predict(img)\n",
    "            probs.append(res)\n",
    "        average_probs = np.mean(np.stack(probs), axis=0)\n",
    "        decision = np.argmax(average_probs, axis=1)\n",
    "        #print(decision)\n",
    "        if decision == y_test_t[i]:\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1      \n",
    "    print(\"Correct predication:\" ,true)\n",
    "    print(\"Incorrect predication:\" ,false)\n",
    "    return true / (true + false)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a8d8c1f-0011-4045-8b97-95595d303c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG19_model = load_model(\"VGG19_t.h5\")\n",
    "DenseNet121_model = load_model(\"DenseNet121_t.h5\")\n",
    "InceptionV3_model = load_model(\"InceptionV3_t.h5\")\n",
    "MobileNet_model = load_model(\"MobileNet_t.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30ea9282-a521-4a9e-bb8a-e84517c54a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predication: 450\n",
      "Incorrect predication: 0\n",
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {voting([VGG19_model,DenseNet121_model,InceptionV3_model,MobileNet_model],X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f76da4-8d91-4a50-ae86-7ba6d74cb1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_combinations(input_list, combination_lengths):\n",
    "    from itertools import combinations\n",
    "    \n",
    "    all_combinations = []\n",
    "    for length in combination_lengths:\n",
    "        all_combinations.extend(list(combinations(input_list, length)))\n",
    "    \n",
    "    return all_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31002c07-0985-4065-9e84-103246d9d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [0, 1, 2, 3]\n",
    "combination_lengths = [2]\n",
    "\n",
    "resulting_combinations = find_combinations(input_list, combination_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273cfe45-fa9e-4fe4-a6bb-fd03c3e50d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [VGG19_model,DenseNet121_model,InceptionV3_model,MobileNet_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2eaf7f94-bdea-44de-b885-5044d542e32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predication: 438\n",
      "Incorrect predication: 12\n",
      "Test Accuracy: 0.9733333333333334\n",
      "Correct predication: 442\n",
      "Incorrect predication: 8\n",
      "Test Accuracy: 0.9822222222222222\n",
      "Correct predication: 437\n",
      "Incorrect predication: 13\n",
      "Test Accuracy: 0.9711111111111111\n",
      "Correct predication: 440\n",
      "Incorrect predication: 10\n",
      "Test Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "for m in model_list:\n",
    "    print(f\"Test Accuracy: {voting([m],X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f1847c6-35b9-41d7-9150-3a9eeb6c0cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predication: 446\n",
      "Incorrect predication: 4\n",
      "Test Accuracy: 0.9911111111111112\n",
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n",
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n",
      "Correct predication: 447\n",
      "Incorrect predication: 3\n",
      "Test Accuracy: 0.9933333333333333\n",
      "Correct predication: 447\n",
      "Incorrect predication: 3\n",
      "Test Accuracy: 0.9933333333333333\n",
      "Correct predication: 447\n",
      "Incorrect predication: 3\n",
      "Test Accuracy: 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "for (x,y) in resulting_combinations:\n",
    "    print(f\"Test Accuracy: {voting([model_list[x],model_list[y]],X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a558648a-d786-40ef-8ac9-3741e3533a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_lengths = [3]\n",
    "resulting_combinations = find_combinations(input_list, combination_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b85b23d-66fa-4c4b-b1a6-ddb05e09e88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n",
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n",
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n",
      "Correct predication: 449\n",
      "Incorrect predication: 1\n",
      "Test Accuracy: 0.9977777777777778\n"
     ]
    }
   ],
   "source": [
    "for (x,y,z) in resulting_combinations:\n",
    "    print(f\"Test Accuracy: {voting([model_list[x],model_list[y],model_list[z]],X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ca764-0ce4-4bb5-9815-c1d8fe5213e0",
   "metadata": {},
   "source": [
    "## Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d902bdf-1e0e-4f44-ab53-95c2527df4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import timm\n",
    "from transformers import ViTFeatureExtractor, TFAutoModelForImageClassification, ViTForImageClassification\n",
    "from transformers import AutoConfig\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee62c578-9ac2-4412-a02a-d6df0f1df8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) \n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4d4b2c0-eee7-4a57-aada-cd7dbd58a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/vit-base-patch16-224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4d908c3-8aa0-48db-a55e-83c10bdfc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1690b948-4d7a-4516-98ee-ff8e7a25d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, feature_extractor):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Convert label to a tensor\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "456bded4-04d0-4a84-b588-73cfcb794f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_list(nparray):\n",
    "    li = []\n",
    "    for i in range(nparray.shape[0]):\n",
    "        img = Image.fromarray(nparray[i].astype('uint8'), 'RGB')\n",
    "        li.append(img)\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "785d4f58-4eeb-4dd2-a6eb-fe501bf7e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = np_to_list(X_train)\n",
    "X_val_raw = np_to_list(X_val)\n",
    "X_test_raw = np_to_list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "198de1c5-f01c-44d8-bafa-e15b3420bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.tolist()\n",
    "y_val = y_val.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15d749d2-840d-46b2-814c-788b803a967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomImageDataset(X_train_raw, y_train, image_processor)\n",
    "val_dataset = CustomImageDataset(X_val_raw, y_val, image_processor)\n",
    "test_dataset = CustomImageDataset(X_test_raw, y_test, image_processor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "102b2ba1-1432-4051-b28a-c4e0aecf8ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.1292, Train Accuracy: 95.85%, Val Loss: 0.3213, Val Accuracy: 90.00%\n",
      "Epoch 2, Train Loss: 0.0359, Train Accuracy: 98.98%, Val Loss: 0.3067, Val Accuracy: 91.56%\n",
      "Epoch 3, Train Loss: 0.0123, Train Accuracy: 99.67%, Val Loss: 0.2642, Val Accuracy: 94.00%\n",
      "Epoch 4, Train Loss: 0.0015, Train Accuracy: 99.99%, Val Loss: 0.2025, Val Accuracy: 95.11%\n",
      "Epoch 5, Train Loss: 0.0004, Train Accuracy: 100.00%, Val Loss: 0.2023, Val Accuracy: 95.56%\n",
      "Epoch 6, Train Loss: 0.0003, Train Accuracy: 100.00%, Val Loss: 0.2072, Val Accuracy: 95.33%\n",
      "Epoch 7, Train Loss: 0.0002, Train Accuracy: 100.00%, Val Loss: 0.2119, Val Accuracy: 95.33%\n",
      "Epoch 8, Train Loss: 0.0002, Train Accuracy: 100.00%, Val Loss: 0.2143, Val Accuracy: 95.33%\n",
      "Epoch 9, Train Loss: 0.0001, Train Accuracy: 100.00%, Val Loss: 0.2193, Val Accuracy: 95.33%\n",
      "Epoch 10, Train Loss: 0.0001, Train Accuracy: 100.00%, Val Loss: 0.2226, Val Accuracy: 95.33%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total_predictions += labels.size(0)\n",
    "            val_correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_accuracy = val_correct_predictions / val_total_predictions * 100\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, \"\n",
    "          f\"Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "508f0f44-4dc7-45c5-b403-6a652900a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set: 95.55555555555556%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of test set: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d12354-a943-44d2-a0cf-41139678fa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_gpu_pytorch",
   "language": "python",
   "name": "env_gpu_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
