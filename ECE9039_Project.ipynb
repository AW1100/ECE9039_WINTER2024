{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92be8215-820b-490a-a5dd-fe96eb846dba",
   "metadata": {},
   "source": [
    "# ECE9039 MACHINE LEARNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57344634-1075-42e1-bf03-415a94a890e4",
   "metadata": {},
   "source": [
    "Temp:\n",
    "- Features Extraction\n",
    "- Autoencoders\n",
    "- MLP\n",
    "- CNN\n",
    "- GANs\n",
    "- Transfer Learning with Pre-trained Models\n",
    "- Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f030b12-cacf-462b-8768-920101b70025",
   "metadata": {},
   "source": [
    "1. Accuracy\n",
    "The most straightforward metric, it measures the proportion of correctly classified instances out of the total instances.\n",
    "\n",
    "2. Precision\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of a classifier's exactness. High precision relates to a low false positive rate.\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate)\n",
    "Recall is the ratio of correctly predicted positive observations to all observations in the actual class. It is a measure of a classifier's completeness.\n",
    "\n",
    "4. F1 Score\n",
    "The F1 Score is the harmonic mean of Precision and Recall. An F1 Score might be a better measure to use if you need to seek a balance between Precision and Recall and there is an uneven class distribution (large number of actual negatives).\n",
    "\n",
    "5. Confusion Matrix\n",
    "A confusion matrix is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows you to visualize the performance of \n",
    "\n",
    "10. Top-k Accuracy\n",
    "In cases where the model can predict multiple classes, the top-k accuracy might be used. It measures whether the correct label is within the top-k predicted labels\n",
    "\n",
    "12. Cross-Entropy Loss\n",
    "Also known as log loss, it measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\n",
    "\n",
    "13. Balanced Accuracy\n",
    "This metric is used for imbalanced datasets and is calculated as the average of recall obtained on each class. It compensates for the fact that traditional accuracy can be skewed when class distributions are imbalanced..an algorithm..teness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a45222-985d-49d3-8fcb-9625be0ab111",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5486ee10-b40e-451c-a785-47e7124e59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image, ImageOps\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce1e49ab-35ed-483c-b7ac-3cca694ce19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(dataset_path, target_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Loop through each subfolder in the dataset directory\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if root == dataset_path:\n",
    "            continue\n",
    "        label = os.path.basename(root)\n",
    "        count = 0\n",
    "        for file in sorted(files):\n",
    "            if file.lower().endswith(\".jpg\"):\n",
    "                # Construct the full path to the image file\n",
    "                file_path = os.path.join(root, file)\n",
    "                image = Image.open(file_path).convert('RGB').resize(target_size)\n",
    "                images.append(np.array(image))\n",
    "                labels.append(label)\n",
    "                count += 1\n",
    "                \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6802bdb1-9b5b-47d8-b3c9-7bb98f2f03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: (4752, 224, 224, 3)\n",
      "Labels count: 9\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'RealWaste/'\n",
    "images, labels = load_images_and_labels(dataset_path)\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels count:\", len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c02f3f3-547d-4727-a13f-887874a2183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(images.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "images = images[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b992015-99fa-4546-b5a9-205b2401f7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal: 790\n",
      "Plastic: 921\n",
      "Paper: 500\n",
      "Cardboard: 461\n",
      "Food Organics: 411\n",
      "Glass: 420\n",
      "Vegetation: 436\n",
      "Miscellaneous Trash: 495\n",
      "Textile Trash: 318\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf0b6019-74c8-40e3-b8b5-e5e51332c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_test_samples_per_class = 100\n",
    "target_num_of_train_per_class = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56de2528-973a-4b3e-b5eb-4de662a66b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(labels)\n",
    "test_indices = []\n",
    "train_indices = []\n",
    "\n",
    "for class_label in classes:\n",
    "    # Find the indices of images belonging to the current class\n",
    "    class_indices = np.where(labels == class_label)[0]\n",
    "    \n",
    "    # Shuffle the indices to ensure random selection\n",
    "    np.random.shuffle(class_indices)\n",
    "    test_indices.extend(class_indices[:num_of_test_samples_per_class])\n",
    "    train_indices.extend(class_indices[num_of_test_samples_per_class:])\n",
    "\n",
    "test_images = images[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "train_images = images[train_indices]\n",
    "train_labels = labels[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59f40f75-9699-4dee-bb36-001fe4e734a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3852, 224, 224, 3)\n",
      "(3852,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "905f5175-98c2-44fd-a172-971083dabc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = None\n",
    "labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "185534be-05df-4946-91ec-ef15c0c86170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardboard: 361\n",
      "Food Organics: 311\n",
      "Glass: 320\n",
      "Metal: 690\n",
      "Miscellaneous Trash: 395\n",
      "Paper: 400\n",
      "Plastic: 821\n",
      "Textile Trash: 218\n",
      "Vegetation: 336\n"
     ]
    }
   ],
   "source": [
    "label_counts = Counter(train_labels)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de8e9fd2-a627-4a97-931c-f95ec40e16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for i in label_counts:\n",
    "    desired = target_num_of_train_per_class-label_counts[i]\n",
    "    dict[i] = desired if desired > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92fea3bc-f311-4969-b7c3-d88b63732716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cardboard': 439, 'Food Organics': 489, 'Glass': 480, 'Metal': 110, 'Miscellaneous Trash': 405, 'Paper': 400, 'Plastic': 0, 'Textile Trash': 582, 'Vegetation': 464}\n",
      "3369\n"
     ]
    }
   ],
   "source": [
    "print(dict)\n",
    "print(sum(dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a87ae34-3565-4635-b299-174826a7ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b669e170-54a7-46a7-85bf-a401ad1280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "\n",
    "def generate_augmented_image(image, label): #,index\n",
    "    for x_batch in datagen.flow(image.reshape((1,)+image.shape),batch_size=1): # ,save_to_dir='Preview', save_prefix=str(index), save_format='jpg'\n",
    "        augmented_images.append(x_batch[0])\n",
    "        augmented_labels.append(label)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c53f1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while sum(dict.values()) > 0:\n",
    "    j = i % len(train_images)\n",
    "    if dict[train_labels[j]] > 0:\n",
    "        dict[train_labels[j]]-=1 \n",
    "        generate_augmented_image(train_images[j],train_labels[j])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6322174f-a222-4558-aaa0-a2375621dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3369, 224, 224, 3)\n",
      "(3369,)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(augmented_images).shape)\n",
    "print(np.array(augmented_labels).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dfe2b66-64f1-45ef-9b60-63ccd497a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.concatenate((train_images,augmented_images))\n",
    "train_labels = np.concatenate((train_labels,augmented_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de915246-b87e-47da-b1fc-a36eeceeeea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7221, 224, 224, 3)\n",
      "(7221,)\n",
      "(900, 224, 224, 3)\n",
      "(900,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2274b48d-6cb0-4826-8b37-aad558d71c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02f547cc-550e-4127-9043-8cd176f79422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7221, 224, 224, 3)\n",
      "(7221,)\n",
      "(450, 224, 224, 3)\n",
      "(450,)\n",
      "(450, 224, 224, 3)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_images\n",
    "y_train = train_labels_encoded\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(test_images, test_labels_encoded, test_size=0.5, random_state=42, stratify=test_labels_encoded)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f7f068a-1fea-4393-a8c3-8eb8f714752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = None\n",
    "train_labels = None\n",
    "train_labels_encoded = None\n",
    "test_images = None\n",
    "test_labels = None\n",
    "test_labels_encoded = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1fc32-5f6a-45c7-ae3c-e73babb9105e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b33792-d1af-4c2e-ab59-6f79c02e24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(256, (3, 3), activation='relu', input_shape=(224,224,3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d56454-dbd0-457e-b126-d4ab80936a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100,validation_data=(X_test, y_test), batch_size=32)\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64152e27-4d6f-40a1-94d7-3a284a103d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss function Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab93d9d-b761-4975-b9e0-7ab238caadc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_gpu)",
   "language": "python",
   "name": "env_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
